{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer (ViT)\n",
        "\n",
        "In this project I am going to work with Vision Transformer. I will start to build our own vit model and train it on an image classification task.\n"
      ],
      "metadata": {
        "id": "nQgfvQ4tT-ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "nFR6WFmfxw43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "xGv2wu1MyAPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65980301-c748-4145-f052-a0a36f6d4c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIT Implementation\n",
        "\n",
        "The vision transformer can be seperated into three parts, we will implement each part and combine them in the end.\n",
        "\n",
        "\n",
        "You can read about the ViT implement from other libary: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py and https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
      ],
      "metadata": {
        "id": "MmNi93C-4rLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchEmbedding\n",
        "PatchEmbedding is responsible for dividing the input image into non-overlapping patches and projecting them into a specified embedding dimension. It uses a 2D convolution layer with a kernel size and stride equal to the patch size. The output is a sequence of linear embeddings for each patch."
      ],
      "metadata": {
        "id": "UNEtT9SQ4jgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "      # TODO\n",
        "      super(PatchEmbedding, self).__init__()\n",
        "      self.image_size = image_size\n",
        "      self.patch_size = patch_size\n",
        "      self.in_channels = in_channels\n",
        "      self.embed_dim = embed_dim\n",
        "      self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "      self.projection_layer = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      # TODO\n",
        "      x = self.projection_layer(x)\n",
        "      x = x.flatten(2)\n",
        "      x = x.transpose(1, 2)\n",
        "      return x"
      ],
      "metadata": {
        "id": "rAzsdK5YybDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadSelfAttention\n",
        "\n",
        "This class implements the multi-head self-attention mechanism, which is a key component of the transformer architecture. It consists of multiple attention heads that independently compute scaled dot-product attention on the input embeddings. This allows the model to capture different aspects of the input at different positions. The attention outputs are concatenated and linearly transformed back to the original embedding size."
      ],
      "metadata": {
        "id": "1mk8v66y6MAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "      # TODO\n",
        "      super(MultiHeadSelfAttention, self).__init__()\n",
        "      self.embed_dim = embed_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.head_dim = embed_dim // num_heads\n",
        "\n",
        "      self.query_layer = nn.Linear(embed_dim, embed_dim)\n",
        "      self.key_layer = nn.Linear(embed_dim, embed_dim)\n",
        "      self.value_layer = nn.Linear(embed_dim, embed_dim)\n",
        "      self.output_layer = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # TODO\n",
        "      batch_size = x.size(0)\n",
        "      query = self.query_layer(x)\n",
        "      key = self.key_layer(x)\n",
        "      value = self.value_layer(x)\n",
        "\n",
        "      queries = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "      keys = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "      values = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "      attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "      attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "      attention_output = torch.matmul(attention_probs, values) #weighted sum\n",
        "      attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
        "      output = self.output_layer(attention_output)\n",
        "      return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V1LeAZq-0dQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TransformerBlock\n",
        "This class represents a single transformer layer. It includes a multi-head self-attention sublayer followed by a position-wise feed-forward network (MLP). Each sublayer is surrounded by residual connections.\n",
        "You may also want to use layer normalization or other type of normalization."
      ],
      "metadata": {
        "id": "NCAURJGJ6jhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        # TODO\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        residual = x\n",
        "        x = self.attention(x)\n",
        "        x = self.layer_norm1(x + residual)\n",
        "        residual = x\n",
        "        x = self.mlp(x)\n",
        "        x = self.layer_norm2(x + residual)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0rT15Biv6igC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VisionTransformer:\n",
        "This is the main class that assembles the entire Vision Transformer architecture. It starts with the PatchEmbedding layer to create patch embeddings from the input image. A special class token is added to the sequence, and positional embeddings are added to both the patch and class tokens. The sequence of patch embeddings is then passed through multiple TransformerBlock layers. The final output is the logits for all classes"
      ],
      "metadata": {
        "id": "rgLfJRUm7EDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        self.patch_embedding = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, self.patch_embedding.num_patches + 1, embed_dim))\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)])\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO\n",
        "        batch_size = x.size(0)\n",
        "        x = self.patch_embedding(x)\n",
        "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat([class_token, x], dim=1)\n",
        "        x = x + self.position_embedding\n",
        "        x = self.dropout(x)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x)\n",
        "        x = x[:, 0]\n",
        "        x = self.mlp_head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tgute9Ab0QP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's train the ViT!\n",
        "\n",
        "We will train the vit to do the image classification with cifar100."
      ],
      "metadata": {
        "id": "lROdKoO37Uqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "image_size = 32\n",
        "patch_size = 4\n",
        "in_channels = 3 #RGB\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "mlp_dim = 512\n",
        "num_layers = 8\n",
        "num_classes = 100\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "byAC841ix_lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout).to(device)\n",
        "input_tensor = torch.randn(1, in_channels, image_size, image_size).to(device)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "1V14TFbM8x4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9698a1-cfd4-4e6a-8110-08987f956b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-100 dataset\n",
        "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
        "transform_train = transforms.Compose([\n",
        "    AutoAugment(policy=AutoAugmentPolicy.CIFAR10),  # AutoAugment for CIFAR-like datasets\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # CIFAR-100 mean and std\n",
        "])\n",
        "\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "3BOp450mdC-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79a8b81-2ae1-4009-debc-691a36497b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:18<00:00, 9.23MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n"
      ],
      "metadata": {
        "id": "4s8-X4l-exSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import accumulate\n",
        "\n",
        "#Traning Loop\n",
        "num_epochs = 100\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0  # Count of correct predictions\n",
        "    total_train = 0    # Total number of samples in training data\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct_train += (predicted == labels).sum().item()  # Sum correct predictions\n",
        "        total_train += labels.size(0)                        # Track total samples\n",
        "    scheduler.step()\n",
        "    # Calculate epoch loss and accuracy\n",
        "    epoch_loss = running_loss / len(trainloader)\n",
        "    epoch_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "    print(f\"Epoch: {epoch + 1}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"New best model saved with validation accuracy: {best_val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "eOyk345ve5HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ce8907-a3cf-4ffc-b4a8-735c51a26939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 4.3502, Training Accuracy: 5.31%\n",
            "Epoch: 1, Validation Accuracy: 10.96%\n",
            "New best model saved with validation accuracy: 10.96%\n",
            "Epoch: 2, Loss: 3.9845, Training Accuracy: 11.38%\n",
            "Epoch: 2, Validation Accuracy: 19.22%\n",
            "New best model saved with validation accuracy: 19.22%\n",
            "Epoch: 3, Loss: 3.7334, Training Accuracy: 16.39%\n",
            "Epoch: 3, Validation Accuracy: 23.11%\n",
            "New best model saved with validation accuracy: 23.11%\n",
            "Epoch: 4, Loss: 3.5512, Training Accuracy: 20.42%\n",
            "Epoch: 4, Validation Accuracy: 27.46%\n",
            "New best model saved with validation accuracy: 27.46%\n",
            "Epoch: 5, Loss: 3.3982, Training Accuracy: 23.83%\n",
            "Epoch: 5, Validation Accuracy: 30.60%\n",
            "New best model saved with validation accuracy: 30.60%\n",
            "Epoch: 6, Loss: 3.2842, Training Accuracy: 26.70%\n",
            "Epoch: 6, Validation Accuracy: 35.17%\n",
            "New best model saved with validation accuracy: 35.17%\n",
            "Epoch: 7, Loss: 3.1932, Training Accuracy: 28.75%\n",
            "Epoch: 7, Validation Accuracy: 36.27%\n",
            "New best model saved with validation accuracy: 36.27%\n",
            "Epoch: 8, Loss: 3.1117, Training Accuracy: 30.97%\n",
            "Epoch: 8, Validation Accuracy: 38.05%\n",
            "New best model saved with validation accuracy: 38.05%\n",
            "Epoch: 9, Loss: 3.0611, Training Accuracy: 32.06%\n",
            "Epoch: 9, Validation Accuracy: 39.04%\n",
            "New best model saved with validation accuracy: 39.04%\n",
            "Epoch: 10, Loss: 3.0264, Training Accuracy: 33.05%\n",
            "Epoch: 10, Validation Accuracy: 39.80%\n",
            "New best model saved with validation accuracy: 39.80%\n",
            "Epoch: 11, Loss: 3.2189, Training Accuracy: 27.94%\n",
            "Epoch: 11, Validation Accuracy: 36.82%\n",
            "Epoch: 12, Loss: 3.1547, Training Accuracy: 29.72%\n",
            "Epoch: 12, Validation Accuracy: 38.79%\n",
            "Epoch: 13, Loss: 3.0861, Training Accuracy: 31.46%\n",
            "Epoch: 13, Validation Accuracy: 39.64%\n",
            "Epoch: 14, Loss: 3.0275, Training Accuracy: 32.93%\n",
            "Epoch: 14, Validation Accuracy: 42.03%\n",
            "New best model saved with validation accuracy: 42.03%\n",
            "Epoch: 15, Loss: 2.9573, Training Accuracy: 34.64%\n",
            "Epoch: 15, Validation Accuracy: 43.91%\n",
            "New best model saved with validation accuracy: 43.91%\n",
            "Epoch: 16, Loss: 2.8972, Training Accuracy: 36.22%\n",
            "Epoch: 16, Validation Accuracy: 45.87%\n",
            "New best model saved with validation accuracy: 45.87%\n",
            "Epoch: 17, Loss: 2.8259, Training Accuracy: 38.08%\n",
            "Epoch: 17, Validation Accuracy: 45.51%\n",
            "Epoch: 18, Loss: 2.7772, Training Accuracy: 39.38%\n",
            "Epoch: 18, Validation Accuracy: 47.51%\n",
            "New best model saved with validation accuracy: 47.51%\n",
            "Epoch: 19, Loss: 2.7249, Training Accuracy: 40.77%\n",
            "Epoch: 19, Validation Accuracy: 48.33%\n",
            "New best model saved with validation accuracy: 48.33%\n",
            "Epoch: 20, Loss: 2.6646, Training Accuracy: 42.22%\n",
            "Epoch: 20, Validation Accuracy: 49.59%\n",
            "New best model saved with validation accuracy: 49.59%\n",
            "Epoch: 21, Loss: 2.6125, Training Accuracy: 44.04%\n",
            "Epoch: 21, Validation Accuracy: 50.87%\n",
            "New best model saved with validation accuracy: 50.87%\n",
            "Epoch: 22, Loss: 2.5594, Training Accuracy: 45.57%\n",
            "Epoch: 22, Validation Accuracy: 52.00%\n",
            "New best model saved with validation accuracy: 52.00%\n",
            "Epoch: 23, Loss: 2.5214, Training Accuracy: 46.55%\n",
            "Epoch: 23, Validation Accuracy: 52.45%\n",
            "New best model saved with validation accuracy: 52.45%\n",
            "Epoch: 24, Loss: 2.4856, Training Accuracy: 47.38%\n",
            "Epoch: 24, Validation Accuracy: 53.47%\n",
            "New best model saved with validation accuracy: 53.47%\n",
            "Epoch: 25, Loss: 2.4422, Training Accuracy: 48.89%\n",
            "Epoch: 25, Validation Accuracy: 53.49%\n",
            "New best model saved with validation accuracy: 53.49%\n",
            "Epoch: 26, Loss: 2.4211, Training Accuracy: 49.31%\n",
            "Epoch: 26, Validation Accuracy: 54.14%\n",
            "New best model saved with validation accuracy: 54.14%\n",
            "Epoch: 27, Loss: 2.3964, Training Accuracy: 50.10%\n",
            "Epoch: 27, Validation Accuracy: 54.35%\n",
            "New best model saved with validation accuracy: 54.35%\n",
            "Epoch: 28, Loss: 2.3774, Training Accuracy: 50.43%\n",
            "Epoch: 28, Validation Accuracy: 54.55%\n",
            "New best model saved with validation accuracy: 54.55%\n",
            "Epoch: 29, Loss: 2.3594, Training Accuracy: 51.15%\n",
            "Epoch: 29, Validation Accuracy: 55.23%\n",
            "New best model saved with validation accuracy: 55.23%\n",
            "Epoch: 30, Loss: 2.3540, Training Accuracy: 51.53%\n",
            "Epoch: 30, Validation Accuracy: 55.32%\n",
            "New best model saved with validation accuracy: 55.32%\n",
            "Epoch: 31, Loss: 2.6078, Training Accuracy: 43.99%\n",
            "Epoch: 31, Validation Accuracy: 49.48%\n",
            "Epoch: 32, Loss: 2.6141, Training Accuracy: 44.10%\n",
            "Epoch: 32, Validation Accuracy: 51.25%\n",
            "Epoch: 33, Loss: 2.5779, Training Accuracy: 44.93%\n",
            "Epoch: 33, Validation Accuracy: 50.80%\n",
            "Epoch: 34, Loss: 2.5438, Training Accuracy: 45.81%\n",
            "Epoch: 34, Validation Accuracy: 50.48%\n",
            "Epoch: 35, Loss: 2.5149, Training Accuracy: 46.50%\n",
            "Epoch: 35, Validation Accuracy: 52.23%\n",
            "Epoch: 36, Loss: 2.4859, Training Accuracy: 47.55%\n",
            "Epoch: 36, Validation Accuracy: 53.12%\n",
            "Epoch: 37, Loss: 2.4566, Training Accuracy: 48.26%\n",
            "Epoch: 37, Validation Accuracy: 53.06%\n",
            "Epoch: 38, Loss: 2.4388, Training Accuracy: 48.64%\n",
            "Epoch: 38, Validation Accuracy: 54.09%\n",
            "Epoch: 39, Loss: 2.4069, Training Accuracy: 49.93%\n",
            "Epoch: 39, Validation Accuracy: 54.70%\n",
            "Epoch: 40, Loss: 2.3685, Training Accuracy: 50.96%\n",
            "Epoch: 40, Validation Accuracy: 53.77%\n",
            "Epoch: 41, Loss: 2.3342, Training Accuracy: 51.81%\n",
            "Epoch: 41, Validation Accuracy: 55.97%\n",
            "New best model saved with validation accuracy: 55.97%\n",
            "Epoch: 42, Loss: 2.3105, Training Accuracy: 52.46%\n",
            "Epoch: 42, Validation Accuracy: 56.47%\n",
            "New best model saved with validation accuracy: 56.47%\n",
            "Epoch: 43, Loss: 2.2863, Training Accuracy: 53.11%\n",
            "Epoch: 43, Validation Accuracy: 57.10%\n",
            "New best model saved with validation accuracy: 57.10%\n",
            "Epoch: 44, Loss: 2.2522, Training Accuracy: 54.62%\n",
            "Epoch: 44, Validation Accuracy: 58.46%\n",
            "New best model saved with validation accuracy: 58.46%\n",
            "Epoch: 45, Loss: 2.2181, Training Accuracy: 55.35%\n",
            "Epoch: 45, Validation Accuracy: 57.57%\n",
            "Epoch: 46, Loss: 2.1931, Training Accuracy: 56.27%\n",
            "Epoch: 46, Validation Accuracy: 58.43%\n",
            "Epoch: 47, Loss: 2.1697, Training Accuracy: 57.00%\n",
            "Epoch: 47, Validation Accuracy: 58.58%\n",
            "New best model saved with validation accuracy: 58.58%\n",
            "Epoch: 48, Loss: 2.1350, Training Accuracy: 58.23%\n",
            "Epoch: 48, Validation Accuracy: 58.47%\n",
            "Epoch: 49, Loss: 2.1061, Training Accuracy: 58.77%\n",
            "Epoch: 49, Validation Accuracy: 59.19%\n",
            "New best model saved with validation accuracy: 59.19%\n",
            "Epoch: 50, Loss: 2.0841, Training Accuracy: 59.66%\n",
            "Epoch: 50, Validation Accuracy: 60.27%\n",
            "New best model saved with validation accuracy: 60.27%\n",
            "Epoch: 51, Loss: 2.0489, Training Accuracy: 60.51%\n",
            "Epoch: 51, Validation Accuracy: 59.55%\n",
            "Epoch: 52, Loss: 2.0295, Training Accuracy: 61.20%\n",
            "Epoch: 52, Validation Accuracy: 59.48%\n",
            "Epoch: 53, Loss: 1.9996, Training Accuracy: 62.17%\n",
            "Epoch: 53, Validation Accuracy: 60.39%\n",
            "New best model saved with validation accuracy: 60.39%\n",
            "Epoch: 54, Loss: 1.9749, Training Accuracy: 62.97%\n",
            "Epoch: 54, Validation Accuracy: 60.90%\n",
            "New best model saved with validation accuracy: 60.90%\n",
            "Epoch: 55, Loss: 1.9538, Training Accuracy: 63.71%\n",
            "Epoch: 55, Validation Accuracy: 61.50%\n",
            "New best model saved with validation accuracy: 61.50%\n",
            "Epoch: 56, Loss: 1.9339, Training Accuracy: 64.48%\n",
            "Epoch: 56, Validation Accuracy: 60.94%\n",
            "Epoch: 57, Loss: 1.9084, Training Accuracy: 65.28%\n",
            "Epoch: 57, Validation Accuracy: 61.30%\n",
            "Epoch: 58, Loss: 1.8888, Training Accuracy: 65.75%\n",
            "Epoch: 58, Validation Accuracy: 61.80%\n",
            "New best model saved with validation accuracy: 61.80%\n",
            "Epoch: 59, Loss: 1.8719, Training Accuracy: 66.55%\n",
            "Epoch: 59, Validation Accuracy: 61.61%\n",
            "Epoch: 60, Loss: 1.8565, Training Accuracy: 66.89%\n",
            "Epoch: 60, Validation Accuracy: 61.94%\n",
            "New best model saved with validation accuracy: 61.94%\n",
            "Epoch: 61, Loss: 1.8398, Training Accuracy: 67.54%\n",
            "Epoch: 61, Validation Accuracy: 61.95%\n",
            "New best model saved with validation accuracy: 61.95%\n",
            "Epoch: 62, Loss: 1.8219, Training Accuracy: 68.11%\n",
            "Epoch: 62, Validation Accuracy: 62.93%\n",
            "New best model saved with validation accuracy: 62.93%\n",
            "Epoch: 63, Loss: 1.8150, Training Accuracy: 68.45%\n",
            "Epoch: 63, Validation Accuracy: 62.68%\n",
            "Epoch: 64, Loss: 1.8019, Training Accuracy: 68.86%\n",
            "Epoch: 64, Validation Accuracy: 62.67%\n",
            "Epoch: 65, Loss: 1.7907, Training Accuracy: 69.24%\n",
            "Epoch: 65, Validation Accuracy: 62.82%\n",
            "Epoch: 66, Loss: 1.7891, Training Accuracy: 69.26%\n",
            "Epoch: 66, Validation Accuracy: 62.86%\n",
            "Epoch: 67, Loss: 1.7797, Training Accuracy: 69.55%\n",
            "Epoch: 67, Validation Accuracy: 63.03%\n",
            "New best model saved with validation accuracy: 63.03%\n",
            "Epoch: 68, Loss: 1.7806, Training Accuracy: 69.49%\n",
            "Epoch: 68, Validation Accuracy: 62.83%\n",
            "Epoch: 69, Loss: 1.7719, Training Accuracy: 69.81%\n",
            "Epoch: 69, Validation Accuracy: 62.89%\n",
            "Epoch: 70, Loss: 1.7759, Training Accuracy: 69.86%\n",
            "Epoch: 70, Validation Accuracy: 62.86%\n",
            "Epoch: 71, Loss: 2.0912, Training Accuracy: 59.07%\n",
            "Epoch: 71, Validation Accuracy: 58.07%\n",
            "Epoch: 72, Loss: 2.1095, Training Accuracy: 58.77%\n",
            "Epoch: 72, Validation Accuracy: 58.75%\n",
            "Epoch: 73, Loss: 2.0914, Training Accuracy: 58.97%\n",
            "Epoch: 73, Validation Accuracy: 58.23%\n",
            "Epoch: 74, Loss: 2.0835, Training Accuracy: 59.40%\n",
            "Epoch: 74, Validation Accuracy: 58.48%\n",
            "Epoch: 75, Loss: 2.0818, Training Accuracy: 59.38%\n",
            "Epoch: 75, Validation Accuracy: 58.53%\n",
            "Epoch: 76, Loss: 2.0596, Training Accuracy: 60.07%\n",
            "Epoch: 76, Validation Accuracy: 58.03%\n",
            "Epoch: 77, Loss: 2.0441, Training Accuracy: 60.51%\n",
            "Epoch: 77, Validation Accuracy: 58.05%\n",
            "Epoch: 78, Loss: 2.0285, Training Accuracy: 61.03%\n",
            "Epoch: 78, Validation Accuracy: 58.77%\n",
            "Epoch: 79, Loss: 2.0203, Training Accuracy: 61.26%\n",
            "Epoch: 79, Validation Accuracy: 58.32%\n",
            "Epoch: 80, Loss: 2.0102, Training Accuracy: 61.53%\n",
            "Epoch: 80, Validation Accuracy: 58.91%\n",
            "Epoch: 81, Loss: 1.9906, Training Accuracy: 62.43%\n",
            "Epoch: 81, Validation Accuracy: 59.25%\n",
            "Epoch: 82, Loss: 1.9769, Training Accuracy: 62.86%\n",
            "Epoch: 82, Validation Accuracy: 59.34%\n",
            "Epoch: 83, Loss: 1.9683, Training Accuracy: 63.27%\n",
            "Epoch: 83, Validation Accuracy: 59.89%\n",
            "Epoch: 84, Loss: 1.9480, Training Accuracy: 63.68%\n",
            "Epoch: 84, Validation Accuracy: 60.61%\n",
            "Epoch: 85, Loss: 1.9352, Training Accuracy: 64.09%\n",
            "Epoch: 85, Validation Accuracy: 60.44%\n",
            "Epoch: 86, Loss: 1.9323, Training Accuracy: 64.30%\n",
            "Epoch: 86, Validation Accuracy: 60.72%\n",
            "Epoch: 87, Loss: 1.9055, Training Accuracy: 65.29%\n",
            "Epoch: 87, Validation Accuracy: 59.04%\n",
            "Epoch: 88, Loss: 1.8935, Training Accuracy: 65.60%\n",
            "Epoch: 88, Validation Accuracy: 60.13%\n",
            "Epoch: 89, Loss: 1.8834, Training Accuracy: 66.11%\n",
            "Epoch: 89, Validation Accuracy: 61.21%\n",
            "Epoch: 90, Loss: 1.8679, Training Accuracy: 66.43%\n",
            "Epoch: 90, Validation Accuracy: 61.01%\n",
            "Epoch: 91, Loss: 1.8611, Training Accuracy: 66.67%\n",
            "Epoch: 91, Validation Accuracy: 60.40%\n",
            "Epoch: 92, Loss: 1.8390, Training Accuracy: 67.29%\n",
            "Epoch: 92, Validation Accuracy: 61.72%\n",
            "Epoch: 93, Loss: 1.8192, Training Accuracy: 67.88%\n",
            "Epoch: 93, Validation Accuracy: 60.91%\n",
            "Epoch: 94, Loss: 1.8068, Training Accuracy: 68.65%\n",
            "Epoch: 94, Validation Accuracy: 61.00%\n",
            "Epoch: 95, Loss: 1.8042, Training Accuracy: 68.51%\n",
            "Epoch: 95, Validation Accuracy: 61.75%\n",
            "Epoch: 96, Loss: 1.7794, Training Accuracy: 69.47%\n",
            "Epoch: 96, Validation Accuracy: 61.80%\n",
            "Epoch: 97, Loss: 1.7698, Training Accuracy: 69.76%\n",
            "Epoch: 97, Validation Accuracy: 62.29%\n",
            "Epoch: 98, Loss: 1.7529, Training Accuracy: 70.36%\n",
            "Epoch: 98, Validation Accuracy: 62.36%\n",
            "Epoch: 99, Loss: 1.7289, Training Accuracy: 71.04%\n",
            "Epoch: 99, Validation Accuracy: 62.35%\n",
            "Epoch: 100, Loss: 1.7288, Training Accuracy: 71.07%\n",
            "Epoch: 100, Validation Accuracy: 62.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please submit your best_model.pth with this notebook. And report the best test results you get."
      ],
      "metadata": {
        "id": "-AfNVj1U9xhk"
      }
    }
  ]
}